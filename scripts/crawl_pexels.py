#!/usr/bin/env python3
"""
åˆæ³•çˆ¬å–Pexels CC0å›¾ç‰‡ï¼ˆéå•†ç”¨ï¼‰
ç”¨é€”ï¼šä»…ç”¨äºä»¥å›¾æœå›¾æ¼”ç¤ºï¼Œéµå®ˆPexels APIæ¡æ¬¾
"""
import os
import requests
import dotenv
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆAPI Keyï¼‰
dotenv.load_dotenv()
API_KEY = os.getenv("PEXELS_API_KEY")
if not API_KEY:
    raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®PEXELS_API_KEY")

# é…ç½®é¡¹ï¼ˆæ–°æ‰‹å¯ä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼‰
SEARCH_KEYWORDS = [  # çˆ¬å–çš„å…³é”®è¯ï¼ˆå¯¹åº”æ¼”ç¤ºæ‰€éœ€çš„å›¾ç‰‡ç±»å‹ï¼‰
    "mug", "desk", "book", "lamp", "keyboard",  # æ—¥å¸¸ç‰©å“ï¼ˆç›¸ä¼¼ç»„ï¼‰
    "mountain", "sky", "leaf", "striped wallpaper", "checkered fabric"  # å¹²æ‰°ç»„
]
PER_KEYWORD = 20  # æ¯ä¸ªå…³é”®è¯çˆ¬å–20å¼ ï¼Œæ€»è®¡200å¼ 
SAVE_DIR = Path("dataset/images")  # ä¿å­˜åˆ°é¡¹ç›®å›¾åº“ç›®å½•
SAVE_DIR.mkdir(parents=True, exist_ok=True)

# Pexels APIåŸºç¡€é…ç½®
BASE_URL = "https://api.pexels.com/v1/search"
HEADERS = {"Authorization": API_KEY}

def download_pexels_image(photo_url, save_path):
    """ä¸‹è½½å•å¼ å›¾ç‰‡ï¼Œå¤„ç†æ ¼å¼ä¸å¤§å°"""
    try:
        # å‘é€è¯·æ±‚ï¼Œè®¾ç½®è¶…æ—¶ä¸æµæ¨¡å¼ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
        response = requests.get(photo_url, stream=True, timeout=10)
        response.raise_for_status()  # æ•è·HTTPé”™è¯¯
        
        # ä¿å­˜å›¾ç‰‡ï¼ˆè‡ªåŠ¨å¤„ç†JPGæ ¼å¼ï¼‰
        with open(save_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"âœ… ä¿å­˜æˆåŠŸï¼š{save_path}")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {photo_url}ï¼š{str(e)}")

def crawl_by_keyword(keyword, num):
    """æŒ‰å…³é”®è¯çˆ¬å–æŒ‡å®šæ•°é‡çš„å›¾ç‰‡"""
    page = 1
    downloaded = 0
    while downloaded < num:
        # æ„é€ APIè¯·æ±‚å‚æ•°ï¼ˆæ¯é¡µæœ€å¤š80å¼ ï¼Œé¿å…é¢‘ç¹è¯·æ±‚ï¼‰
        params = {
            "query": keyword,
            "per_page": min(num - downloaded, 80),
            "page": page,
            "size": "medium"  # ä¸­ç­‰åˆ†è¾¨ç‡ï¼ˆ800Ã—800+ï¼Œé€‚åˆç‰¹å¾æå–ï¼‰
        }
        
        # å‘é€APIè¯·æ±‚ï¼ˆéµå®ˆé¢‘ç‡é™åˆ¶ï¼šâ‰¤200æ¬¡/å°æ—¶ï¼‰
        response = requests.get(BASE_URL, headers=HEADERS, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        # éå†å›¾ç‰‡å¹¶ä¸‹è½½
        for photo in data["photos"]:
            if downloaded >= num:
                break
            # å–ä¸­ç­‰åˆ†è¾¨ç‡çš„å›¾ç‰‡URLï¼ˆå¹³è¡¡è´¨é‡ä¸ä½“ç§¯ï¼‰
            img_url = photo["src"]["medium"]
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼ˆé¿å…é‡å¤ï¼‰
            img_name = f"{keyword}_{downloaded+1}.jpg"
            save_path = SAVE_DIR / img_name
            # ä¸‹è½½å›¾ç‰‡
            download_pexels_image(img_url, save_path)
            downloaded += 1
        
        page += 1
        # æ— æ›´å¤šç»“æœåˆ™åœæ­¢
        if not data["photos"]:
            break

if __name__ == "__main__":
    # éå†å…³é”®è¯çˆ¬å–
    for keyword in SEARCH_KEYWORDS:
        print(f"\n===== å¼€å§‹çˆ¬å–å…³é”®è¯ï¼š{keyword} =====")
        crawl_by_keyword(keyword, PER_KEYWORD)
    print("\nğŸ‰ æ‰€æœ‰å›¾ç‰‡çˆ¬å–å®Œæˆï¼Œä¿å­˜è‡³ï¼š", SAVE_DIR)